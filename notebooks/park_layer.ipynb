{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBTAINING PARK LAYER\n",
    "The goal of this notebook is to produce a final urban green areas layer to performe accessibility analysis. <br>\n",
    "I will use as a baseline the R03土地利用現況 file, which authored by Tokyo Metropolitan Government (opendata.metro.tokyo).The issue with these data is that they include also cemeteries and sports facilities as parks. My solution is the following:\n",
    "- Extract from the land use data the parks (LU_1: 300)\n",
    "- Query OSM for cemeteries, graveyeards, pitches and sport centers.\n",
    "- First merge (dissolve) the extracted land park polygons\n",
    "- Exclude the UGS polygons which overlap to the OSM layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "C:\\Users\\lucap\\Documents\\thesis_work\\data\\ugs\\R03\\R03土地利用現況.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataSourceError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m unwanted_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mlucap\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mthesis_work\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mugs\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124munwanted_features.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# obtained through OSM\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# TODO Research why it takes so long to query over Tokyo OSM -> That's why I used QGIS.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# TODO fix the absolute paths and change the data from shp to GeoJSON or parquet. ## turns out parquet files are slow for geospatial operations\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m landuse \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mugs_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# layer of Tokyo landuse\u001b[39;00m\n\u001b[0;32m      8\u001b[0m unwanted \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_file(unwanted_path) \u001b[38;5;66;03m# layer of cemeteries, graveyeards and sport facilities\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\miniconda3\\envs\\spatial\\Lib\\site-packages\\geopandas\\io\\file.py:294\u001b[0m, in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m             from_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_file_like(filename):\n",
      "File \u001b[1;32mc:\\Users\\Luca\\miniconda3\\envs\\spatial\\Lib\\site-packages\\geopandas\\io\\file.py:547\u001b[0m, in \u001b[0;36m_read_file_pyogrio\u001b[1;34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keywords are deprecated, and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future release. You can use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    543\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    544\u001b[0m     )\n\u001b[0;32m    545\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\miniconda3\\envs\\spatial\\Lib\\site-packages\\pyogrio\\geopandas.py:265\u001b[0m, in \u001b[0;36mread_dataframe\u001b[1;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime_as_string\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[0;32m    285\u001b[0m     meta, table \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32mc:\\Users\\Luca\\miniconda3\\envs\\spatial\\Lib\\site-packages\\pyogrio\\raw.py:198\u001b[0m, in \u001b[0;36mread\u001b[1;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mpyogrio\\\\_io.pyx:1240\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpyogrio\\\\_io.pyx:220\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDataSourceError\u001b[0m: C:\\Users\\lucap\\Documents\\thesis_work\\data\\ugs\\R03\\R03土地利用現況.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "# TODO get the relevant files to replicate the procedure\n",
    "ugs_path = \"C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\R03\\\\R03土地利用現況.shp\" # available only locally \n",
    "unwanted_path = \"C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\unwanted_features.geojson\" # obtained through OSM\n",
    "# TODO Research why it takes so long to query over Tokyo OSM -> That's why I used QGIS.\n",
    "# TODO fix the absolute paths and change the data from shp to GeoJSON or parquet. ## turns out parquet files are slow for geospatial operations\n",
    "\n",
    "landuse = gpd.read_file(ugs_path) # layer of Tokyo landuse\n",
    "unwanted = gpd.read_file(unwanted_path) # layer of cemeteries, graveyeards and sport facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess parks data\n",
    "- Fix CRS\n",
    "- Merge adjecent parks\n",
    "- Create index and recompute areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original CRS: EPSG:6677\n",
      "geometry attribute name: geometry\n"
     ]
    }
   ],
   "source": [
    "print(f\"original CRS: {landuse.crs}\") # originally the data is in EPSG:6677\n",
    "landuse = landuse.to_crs(epsg=32654) # I do this because I need a CRS that keeps information about distance to compute the buffers\n",
    "unwanted = unwanted.to_crs(epsg=32654)\n",
    "print(f\"geometry attribute name: {landuse.geometry.name}\") # this gives the name of the attibute corresponding to the geometry column (a GeoSeries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total polygons in the land usage dataset: 815736\n",
      "Number of parks' polygons: 14030\n"
     ]
    }
   ],
   "source": [
    "# filter only the parks from the land use dataframe\n",
    "print(f\"Total polygons in the land usage dataset: {landuse.shape[0]}\")\n",
    "parks = landuse[landuse[\"LU_1\"] == 300].copy() # 300 identifies parks\n",
    "print(f\"Number of parks' polygons: {parks.shape[0]}\")\n",
    "\n",
    "# create a new index and update the areas\n",
    "parks['park_id'] = range(1,len(parks)+1)\n",
    "parks.set_index(parks.park_id)\n",
    "parks.loc[:,'AREA'] = parks.geometry.area\n",
    "parks = parks.rename(columns={'AREA':'area'})\n",
    "initial_parks = parks.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove unwanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1546 parks removed\n"
     ]
    }
   ],
   "source": [
    "# remove features that are mostly unwanted\n",
    "overlap = gpd.overlay(parks, unwanted, how='intersection')\n",
    "overlap['overlap_area'] = overlap.geometry.area\n",
    "tot_overlap = overlap.groupby('park_id')['overlap_area'].sum().reset_index()\n",
    "parks = parks.merge(tot_overlap, on='park_id', how='left')\n",
    "parks['overlap_area'] = parks['overlap_area'].fillna(0)\n",
    "parks['ov_percentage'] = (parks['overlap_area']/parks['area'])* 100\n",
    "filtered_parks = parks[parks['ov_percentage'] <= 50]\n",
    "parks_after_removal = filtered_parks.shape[0]\n",
    "n_parks_removed1 = initial_parks - parks_after_removal\n",
    "print(f\"{n_parks_removed1} parks removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell block used to be above (before checking for unwanted areas)\n",
    "I will now try to keep it here to see whether it is better. The rationale is the following: <br>\n",
    "The following process dissolves and explodes the UGS. Therefore by desing the average area of the parks will increase after the process.\n",
    "It is therefore possible that by moving this process _after_ removing unwanted polygons, more unwanted polygons will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parks' after merging: 10517\n",
      "Step 1 eliminated 1546\n",
      "Step 2 eliminated 1967\n"
     ]
    }
   ],
   "source": [
    "# merge the adjcent parks into a single entity\n",
    "filtered_parks = filtered_parks.dissolve()\n",
    "filtered_parks = filtered_parks.explode()\n",
    "print(f\"Number of parks' after merging: {filtered_parks.shape[0]}\")\n",
    "print(f\"Step 1 eliminated {n_parks_removed1}\")\n",
    "print(f\"Step 2 eliminated {parks_after_removal - filtered_parks.shape[0]}\")\n",
    "filtered_parks.loc[:,'area'] = filtered_parks.geometry.area # the value changed after merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Include bodies of water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main idea: a body of water should be included only if most of its perimeter touches a park. <br>\n",
    "This would allow for ponds within parks to be included into the park itself, however huge rivers would be omitted. <br>\n",
    "Some complications:\n",
    "- Parks can be divided into multiple polygons, so it is important to consider that the total perimiter touching all polygons that compose the park is what matters to evaluate a bluespace's inclusion in the dataset.\n",
    "- My solution is dissolving all parks in one single polygon. However bluespace that touches multiple parks may wrongly be added to the dataset.\n",
    "\n",
    "HOWEVER: many smaller bodies of water are already included into the parks' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parks after joining the bodies of water: 11160\n"
     ]
    }
   ],
   "source": [
    "# Dissolve the parks into a single geometry (unified park boundary)\n",
    "unipark = filtered_parks.dissolve()\n",
    "\n",
    "# Create a single boundary geometry for the parks\n",
    "park_boundary = unipark.geometry.boundary.union_all()  # This creates a single LineString\n",
    "\n",
    "# Apply buffering to the park boundary (adjust buffer size as needed, e.g., 10 meters)\n",
    "buffered_park_boundary = park_boundary.buffer(10) # I checked both 2 and 10 and I find 10 meters more suitable.\n",
    "\n",
    "# Filter the bodies of water (LU_1 = 700) and ensure it's a copy\n",
    "bluespace = landuse[landuse['LU_1'] == 700].copy()\n",
    "\n",
    "# Calculate the boundary of each body of water\n",
    "bluespace.loc[:, 'boundary'] = bluespace.geometry.boundary\n",
    "\n",
    "# Intersect the buffered park boundary with the water feature boundaries\n",
    "bluespace.loc[:, 'shared_boundary'] = bluespace['boundary'].apply(lambda b: b.intersection(buffered_park_boundary))\n",
    "\n",
    "# Calculate the length of the shared boundary for each water feature\n",
    "bluespace.loc[:, 'shared_length'] = bluespace['shared_boundary'].apply(lambda b: b.length)\n",
    "\n",
    "# Calculate the percentage of the water feature's boundary that is shared with the buffered park boundary\n",
    "bluespace.loc[:, 'perimeter'] = bluespace['boundary'].apply(lambda b: b.length)\n",
    "bluespace.loc[:, 'shared_percentage'] = (bluespace['shared_length'] / bluespace['perimeter']) * 100\n",
    "\n",
    "# Filter water features where at least 80% of the boundary is adjacent to the park boundary or its buffer\n",
    "adj_bluespace = bluespace[bluespace['shared_percentage'] >= 80]\n",
    "\n",
    "# Optional: Add these selected water features back to the parks dataset\n",
    "parks_with_water = gpd.GeoDataFrame(pd.concat([filtered_parks, adj_bluespace], ignore_index=True))\n",
    "\n",
    "parks_with_water = parks_with_water.drop(columns=['boundary', 'shared_boundary'])\n",
    "\n",
    "print(f\"Number of parks after joining the bodies of water: {parks_with_water.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add missing greenspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visualizing the data in QGIS it can be noted that shrines and temples' greenery is not included into the dataset. This could be an acceptable limitation to the dataset. However, I believe some areas particularly relevant areas should be included in the UGS dataset. <br>\n",
    "Some examples are:\n",
    "- Imperial Palace East National Gardens\n",
    "- Meji Jingu Gyoen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add new polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parks_data = [\n",
    "    {\n",
    "        \"name\": \"Imperial palace east naional gardens\",\n",
    "        \"geometry\": Polygon([\n",
    "            (387234.5917162087, 3950150.3966461713),\n",
    "            (387281.6385052205, 3950168.129666645),\n",
    "            (387281.6385052205, 3950175.367634185),\n",
    "            (387299.7334240712, 3950182.06275416),\n",
    "            (387296.6572878666, 3950192.91970547),\n",
    "            (387359.6276054669, 3950224.314389676),\n",
    "            (387368.4941157038, 3950208.3908610875), \n",
    "            (387420.78843118215, 3950227.7524242587),\n",
    "            (387424.22646576376, 3950215.266930252),\n",
    "            (387469.1018645135, 3950223.7715421114),\n",
    "            (387476.70173043077, 3950191.743535746),\n",
    "            (387572.785749528, 3950213.909811337),\n",
    "            (387577.95620084833, 3950253.8716067197),\n",
    "            (387573.0173003257, 3950255.1743741767),\n",
    "            (387577.77350422693, 3950275.066383232),\n",
    "            (387581.29547903966, 3950274.404584143),\n",
    "            (387581.8662668961, 3950276.1576717636),\n",
    "            (387631.67757471907, 3950268.3064957964),\n",
    "            (387637.2036606363, 3950244.309275006),\n",
    "            (387753.7349380347, 3950238.5189009737),\n",
    "            (387716.0975068253, 3950056.845915713), \n",
    "            (387751.5635477726, 3950052.5031351885),\n",
    "            (387735.64001918404, 3949914.2579551693),\n",
    "            (387653.1271892249, 3949929.457687004),\n",
    "            (387571.3381560198, 3949601.57775743),\n",
    "            (387418.6170409201, 3949622.5678632963), \n",
    "            (387418.6170409201, 3949646.091257802),\n",
    "            (387380.97960971063, 3949696.0332338302), \n",
    "            (387391.83656102105, 3949709.7853721566), \n",
    "            (387197.1352341878, 3950023.189366651), \n",
    "            (387239.11544592143, 3950051.779338435), \n",
    "            (387213.05876277643, 3950118.3686398054), \n",
    "            (387240.5630394295, 3950130.6731846235), \n",
    "            (387234.5917162087, 3950150.3966461713)]),\n",
    "\n",
    "        \"area\": None,  # Area will be calculated later\n",
    "        \"NAME1\" : \"千代田区\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Kosuge west park\",\n",
    "        \"geometry\": Polygon([\n",
    "            (393136.6516049383, 3957214.7029571617), \n",
    "            (393285.64975817315, 3957210.4108292903), \n",
    "            (393282.8564686064, 3957110.3974369955), \n",
    "            (393215.1362288645, 3957110.12492094), \n",
    "            (393214.18242267094, 3957088.868668627), \n",
    "            (393170.8523698784, 3957090.640022986), \n",
    "            (393172.8962402931, 3957166.126970304), \n",
    "            (393135.42528268945, 3957167.2170345252), \n",
    "            (393136.6516049383, 3957214.7029571617)\n",
    "            ]),\n",
    "        \"area\": None,\n",
    "        \"NAME1\": \"葛飾区\", # compute later         \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Kosuge east sports park\",\n",
    "        \"geometry\": Polygon([\n",
    "            (393470.2250258569, 3957303.2624996495),\n",
    "            (393600.7602163453, 3957321.248559299),\n",
    "            (393624.74162921164, 3957120.267968516),   \n",
    "            (393493.66140661266, 3957104.4620373086), \n",
    "            (393470.2250258569, 3957303.2624996495)\n",
    "            ]),\n",
    "        \"area\": None,\n",
    "        \"NAME1\": \"葛飾区\", # compute later         \n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parks = gpd.GeoDataFrame(new_parks_data)\n",
    "new_parks = new_parks.set_crs(epsg=32654)\n",
    "new_parks = new_parks.to_crs(parks_with_water.crs)\n",
    "new_parks[\"area\"] = new_parks.geometry.area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract 'missclassified' parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse.info() # AREA and CODE5 are numeric\n",
    "misclassified_data = [\n",
    "    {\"area\": 701833.77077, \"CODE5\": 13113032000, \"NAME1\": \"渋谷区\", \"NAME2\": \"代々木神園町\"},\n",
    "    {\"area\": 12442.61194, \"CODE5\": 13120026004, \"NAME1\": \"練馬区\", \"NAME2\": \"土支田四丁目\"},\n",
    "    {\"area\": 1617.28064, \"CODE5\": 13120026004, \"NAME1\": \"練馬区\", \"NAME2\": \"土支田四丁目\"},\n",
    "    {\"area\": 26105.87451, \"CODE5\": 13115021001, \"NAME1\": \"杉並区\", \"NAME2\": \"善福寺一丁目\"},\n",
    "    {\"area\": 16520.83983, \"CODE5\": 13119046002, \"NAME1\": \"板橋区\", \"NAME2\": \"舟渡二丁目\"},\n",
    "    {\"area\": 21991.04371, \"CODE5\": 13117007002, \"NAME1\": \"北区\", \"NAME2\": \"浮間二丁目\"},\n",
    "    {\"area\": 19574.51866, \"CODE5\": 13112019004, \"NAME1\": \"世田谷区\", \"NAME2\": \"喜多見四丁目\"},\n",
    "    {\"area\": 30062.09595, \"CODE5\": 13112007002, \"NAME1\": \"世田谷区\", \"NAME2\": \"岡本二丁目\"},\n",
    "    {\"area\": 3847.82543, \"CODE5\": 13112007002, \"NAME1\": \"世田谷区\", \"NAME2\": \"岡本二丁目\"},\n",
    "    {\"area\": 3133.72299, \"CODE5\": 13112007002, \"NAME1\": \"世田谷区\", \"NAME2\": \"岡本二丁目\"},\n",
    "    {\"area\": 35459.23117, \"CODE5\": 13111055002, \"NAME1\": \"大田区\", \"NAME2\": \"南千束二丁目\"},\n",
    "    {\"area\": 1191.98565, \"CODE5\": 13111055002, \"NAME1\": \"大田区\", \"NAME2\": \"南千束二丁目\"},\n",
    "    {\"area\": 817.15830, \"CODE5\": 13111055002, \"NAME1\": \"大田区\", \"NAME2\": \"南千束二丁目\"},\n",
    "    {\"area\": 3360.63220, \"CODE5\": 13111055002, \"NAME1\": \"大田区\", \"NAME2\": \"南千束二丁目\"},\n",
    "    {\"area\": 5317.68497, \"CODE5\": 13111026005, \"NAME1\": \"大田区\", \"NAME2\": \"中央五丁目\"},\n",
    "    {\"area\": 4569.40623, \"CODE5\": 13111026005, \"NAME1\": \"大田区\", \"NAME2\": \"中央五丁目\"},\n",
    "    {\"area\": 1638.38160, \"CODE5\": 13111026005, \"NAME1\": \"大田区\", \"NAME2\": \"中央五丁目\"}\n",
    "]\n",
    "\n",
    "misclassified_df = pd.DataFrame(misclassified_data)\n",
    "\n",
    "landuse = landuse.rename(columns={\"AREA\":\"area\"})\n",
    "\n",
    "matching_polygons = landuse.merge(\n",
    "    misclassified_df,\n",
    "    on=[\"area\", \"CODE5\", \"NAME1\", \"NAME2\"],  # Match these columns\n",
    "    how=\"inner\"\n",
    ")\n",
    " \n",
    "print(f\"Number of matches: {matching_polygons.shape[0]} out of {len(misclassified_data)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add new data to the park layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_parks = gpd.GeoDataFrame(pd.concat([parks_with_water, matching_polygons], ignore_index=True))\n",
    "final_parks = gpd.GeoDataFrame(pd.concat([final_parks, new_parks], ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finalize the park layer\n",
    "- Dissolve & Explode\n",
    "- Recompute area\n",
    "- Drop usless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless columns\n",
    "final_parks = final_parks.loc[:, [\"area\",\"CODE5\",\"NAME1\",\"NAME2\",\"geometry\"]]\n",
    "n_parks_after_addition = final_parks.shape[0]\n",
    "\n",
    "# reduce number of parks by merging adjacent polygons\n",
    "final_parks = final_parks.dissolve()\n",
    "final_parks = final_parks.explode()\n",
    "\n",
    "# recompute area after merging polygons\n",
    "final_parks['area'] = final_parks.geometry.area\n",
    "f_n_parks = final_parks.shape[0]\n",
    "print(f\"The final number of parks is: {f_n_parks}\")\n",
    "print(f\"Polygons assimilated by joining: {n_parks_after_addition - f_n_parks}\")\n",
    "\n",
    "final_parks[\"park_id\"] = range(1, len(final_parks)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge park collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is written a few weeks after the previous part of the notebook. I got rid of the to_file function, so there is not output being exported. For efficiency reasons I use as UGS layer the one present in the geopackage Tokyo_ugs_accessibility (which is produced by the notebook above). <br>\n",
    "I noticed that some big parks (e.g., Ueno) are composed by multiple smaller park polygons. While this does not directly affect the accessibility measure critically, it will have consequences when I will compute the accessibility for different park size categories. <br>\n",
    "While I thought about different potential solution, the most efficient seems to be the following:\n",
    "- create a 5m buffer around parks\n",
    "- dissolve the buffered layer parks\n",
    "- explode the parks (from single to multiple polygons)\n",
    "- create a negative 5m buffer\n",
    "- recompute park areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugs_path = \"..\\\\data\\\\final\\\\tokyo_ugs_accessibility.gpkg\"\n",
    "ugs = gpd.read_file(ugs_path, layer = 'ugs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 10451 green spaces\n"
     ]
    }
   ],
   "source": [
    "print(f\"I have {ugs.shape[0]} green spaces\")\n",
    "buff_ugs =  ugs.copy()\n",
    "buff_ugs['geometry'] = buff_ugs.buffer(5) # If I do .buffer I get a geoseries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I buffer, dissolve, explode and debuffer\n",
      "After the procedure I have 8430 parks\n"
     ]
    }
   ],
   "source": [
    "buff_ugs = buff_ugs.dissolve()\n",
    "merged_ugs = buff_ugs.explode()\n",
    "merged_ugs['geometry'] = merged_ugs['geometry'].buffer(-5)\n",
    "\n",
    "print(f\"I buffer, dissolve, explode and debuffer\")\n",
    "print(f\"After the procedure I have {merged_ugs.shape[0]} parks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ugs[\"park_id\"] = range(1, len(merged_ugs) + 1)\n",
    "merged_ugs['area'] = merged_ugs.geometry.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ugs.to_file(\"../data/final/tokyo_ugs_accessibility.gpkg\", layer='merged_ugs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
