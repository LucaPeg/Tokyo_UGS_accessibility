{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBTAINING PARK LAYER\n",
    "The goal of this notebook is to produce a final urban green areas layer to performe accessibility analysis. <br>\n",
    "I will use as a baseline the R03土地利用現況 file, which authored by Tokyo Metropolitan Government (opendata.metro.tokyo).The issue with these data is that they include also cemeteries and sports facilities as parks. My solution is the following:\n",
    "- Extract from the land use data the parks (LU_1: 300)\n",
    "- Query OSM for cemeteries, graveyeards, pitches and sport centers.\n",
    "- First merge (dissolve) the extracted land park polygons\n",
    "- Exclude the UGS polygons which overlap to the OSM layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugs_path = \"C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\R03\\\\R03土地利用現況.shp\"\n",
    "unwanted_path = \"C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\unwanted_features.geojson\" # obtained through OSM\n",
    "landuse = gpd.read_file(ugs_path) # layer of Tokyo landuse\n",
    "unwanted = gpd.read_file(unwanted_path) # layer of cemeteries, graveyeards and sport facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess parks data\n",
    "- Fix CRS\n",
    "- Merge adjecent parks\n",
    "- Create index and recompute areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original CRS: EPSG:6677\n",
      "geometry attribute name: geometry\n"
     ]
    }
   ],
   "source": [
    "print(f\"original CRS: {landuse.crs}\") # originally the data is in EPSG:6677\n",
    "landuse = landuse.to_crs(epsg=32654) # I do this because I need a CRS that keeps information about distance to compute the buffers\n",
    "unwanted = unwanted.to_crs(epsg=32654)\n",
    "print(f\"geometry attribute name: {landuse.geometry.name}\") # this gives the name of the attibute corresponding to the geometry column (a GeoSeries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total polygons in the land usage dataset: 815736\n",
      "Number of parks' polygons: 14030\n",
      "Number of parks' after merging: 11830\n"
     ]
    }
   ],
   "source": [
    "# filter only the parks from the land use dataframe\n",
    "print(f\"Total polygons in the land usage dataset: {landuse.shape[0]}\")\n",
    "parks = landuse[landuse[\"LU_1\"] == 300] # 300 identifies parks\n",
    "print(f\"Number of parks' polygons: {parks.shape[0]}\")\n",
    "\n",
    "# merge the adjcent parks into a single entity\n",
    "parks = parks.dissolve()\n",
    "parks = parks.explode()\n",
    "print(f\"Number of parks' after merging: {parks.shape[0]}\")\n",
    "parks_after_merge = parks.shape[0]\n",
    "\n",
    "# create a new index and update the areas\n",
    "parks['park_id'] = range(1,len(parks)+1)\n",
    "parks.set_index(parks.park_id)\n",
    "parks['AREA'] = parks.geometry.area\n",
    "parks = parks.rename(columns={'AREA':'area'})\n",
    "\n",
    "# Save the result\n",
    "#parks.to_file('path_to_output.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove unwanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415 parks removed\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\filtered_parks.geojson'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_parks_removed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parks removed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# export filtered data\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mfiltered_parks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mlucap\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDocuments\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mthesis_work\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mugs\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mfiltered_parks.geojson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucap\\anaconda3\\envs\\spatial\\Lib\\site-packages\\geopandas\\geodataframe.py:1536\u001b[0m, in \u001b[0;36mGeoDataFrame.to_file\u001b[1;34m(self, filename, driver, schema, index, **kwargs)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Write the ``GeoDataFrame`` to a file.\u001b[39;00m\n\u001b[0;32m   1442\u001b[0m \n\u001b[0;32m   1443\u001b[0m \u001b[38;5;124;03mBy default, an ESRI shapefile is written, but any OGR data source\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1532\u001b[0m \n\u001b[0;32m   1533\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _to_file\n\u001b[1;32m-> 1536\u001b[0m \u001b[43m_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucap\\anaconda3\\envs\\spatial\\Lib\\site-packages\\geopandas\\io\\file.py:686\u001b[0m, in \u001b[0;36m_to_file\u001b[1;34m(df, filename, driver, schema, index, mode, crs, engine, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 686\u001b[0m     \u001b[43m_to_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    688\u001b[0m     _to_file_fiona(df, filename, driver, schema, crs, mode, metadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lucap\\anaconda3\\envs\\spatial\\Lib\\site-packages\\geopandas\\io\\file.py:748\u001b[0m, in \u001b[0;36m_to_file_pyogrio\u001b[1;34m(df, filename, driver, schema, crs, mode, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeoDataFrame cannot contain duplicated column names.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mpyogrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucap\\anaconda3\\envs\\spatial\\Lib\\site-packages\\pyogrio\\geopandas.py:662\u001b[0m, in \u001b[0;36mwrite_dataframe\u001b[1;34m(df, path, layer, driver, encoding, geometry_type, promote_to_multi, nan_as_null, append, use_arrow, dataset_metadata, layer_metadata, metadata, dataset_options, layer_options, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m geometry_column \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    660\u001b[0m     geometry \u001b[38;5;241m=\u001b[39m to_wkb(geometry\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m--> 662\u001b[0m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucap\\anaconda3\\envs\\spatial\\Lib\\site-packages\\pyogrio\\raw.py:723\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(path, geometry, field_data, fields, field_mask, layer, driver, geometry_type, crs, encoding, promote_to_multi, nan_as_null, append, dataset_metadata, layer_metadata, metadata, dataset_options, layer_options, gdal_tz_offsets, **kwargs)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;66;03m# preprocess kwargs and split in dataset and layer creation options\u001b[39;00m\n\u001b[0;32m    719\u001b[0m dataset_kwargs, layer_kwargs \u001b[38;5;241m=\u001b[39m _preprocess_options_kwargs(\n\u001b[0;32m    720\u001b[0m     driver, dataset_options, layer_options, kwargs\n\u001b[0;32m    721\u001b[0m )\n\u001b[1;32m--> 723\u001b[0m \u001b[43mogr_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mpyogrio\\\\_io.pyx:2307\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_write\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpyogrio\\\\_io.pyx:2102\u001b[0m, in \u001b[0;36mpyogrio._io.create_ogr_dataset_layer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\filtered_parks.geojson'"
     ]
    }
   ],
   "source": [
    "# remove features that are mostly unwanted\n",
    "overlap = gpd.overlay(parks, unwanted, how='intersection')\n",
    "overlap['overlap_area'] = overlap.geometry.area\n",
    "tot_overlap = overlap.groupby('park_id')['overlap_area'].sum().reset_index()\n",
    "parks = parks.merge(tot_overlap, on='park_id', how='left')\n",
    "parks['overlap_area'] = parks['overlap_area'].fillna(0)\n",
    "parks['ov_percentage'] = (parks['overlap_area']/parks['area'])* 100\n",
    "filtered_parks = parks[parks['ov_percentage'] <= 50]\n",
    "n_parks_removed = parks_after_merge - filtered_parks.shape[0]\n",
    "print(f\"{n_parks_removed} parks removed\")\n",
    "\n",
    "# export filtered data\n",
    "filtered_parks.to_file(\"C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\filtered_parks.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt to include bodies of water\n",
    "THIS IS COMPELTELY WRONG NOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main idea: a body of water should be included only if most of its perimeter touches the parks. <br>\n",
    "This would allow for ponds within parks to be included into the park itself, however huge rivers would be omitted. <br>\n",
    "Some complications:\n",
    "- Parks can be divided into multiple polygons, so it is important to consider that the total perimiter touching all polygons that compose the park is what matters to evaluate a bluespace's inclusion in the dataset.\n",
    "- My solution is dissolving all parks in one single polygon. However bluespace that touches multiple parks may wrongly be added to the dataset.\n",
    "\n",
    "HOWEVER: many smaller bodies of water are already included into the parks' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve the parks into a single geometry (unified park boundary)\n",
    "unipark = filtered_parks.dissolve()\n",
    "\n",
    "# Create a single boundary geometry for the parks\n",
    "park_boundary = unipark.geometry.boundary.union_all()  # This creates a single LineString or MultiLineString\n",
    "\n",
    "# Apply buffering to the park boundary (adjust buffer size as needed, e.g., 10 meters)\n",
    "buffered_park_boundary = park_boundary.buffer(2)\n",
    "\n",
    "# Filter the bodies of water (LU_1 = 700)\n",
    "bluespace = landuse[landuse['LU_1'] == 700]\n",
    "\n",
    "# Calculate the boundary of each body of water\n",
    "bluespace['boundary'] = bluespace.geometry.boundary\n",
    "\n",
    "# Intersect the buffered park boundary with the water feature boundaries\n",
    "bluespace['shared_boundary'] = bluespace['boundary'].apply(lambda b: b.intersection(buffered_park_boundary))\n",
    "\n",
    "# Calculate the length of the shared boundary for each water feature\n",
    "bluespace['shared_length'] = bluespace['shared_boundary'].apply(lambda b: b.length)\n",
    "\n",
    "# Calculate the percentage of the water feature's boundary that is shared with the buffered park boundary\n",
    "bluespace['perimeter'] = bluespace['boundary'].apply(lambda b: b.length)\n",
    "bluespace['shared_percentage'] = (bluespace['shared_length'] / bluespace['perimeter']) * 100\n",
    "\n",
    "# Filter water features where at least 80% of the boundary is adjacent to the park boundary or its buffer\n",
    "adj_bluespace = bluespace[bluespace['shared_percentage'] >= 80]\n",
    "\n",
    "# Optional: Add these selected water features back to the parks dataset\n",
    "parks_with_water = gpd.GeoDataFrame(pd.concat([filtered_parks, adj_bluespace], ignore_index=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the first iteration of adding the bodies of water inside parks. <br>\n",
    "It worked, but without buffering \"only\" 200+ polygons were added. Some important absents were the Ueno park's ponds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve the parks into a single geometry (unified park boundary)\n",
    "#unipark = filtered_parks.dissolve()\n",
    "\n",
    "# Create a single boundary geometry for the parks\n",
    "#park_boundary = unipark.geometry.boundary.union_all()  # This creates a single LineString or MultiLineString\n",
    "\n",
    "# Filter the bodies of water (LU_1 = 700)\n",
    "#bluespace = landuse[landuse['LU_1'] == 700]\n",
    "\n",
    "# Calculate the boundary of each body of water\n",
    "#bluespace['boundary'] = bluespace.geometry.boundary\n",
    "\n",
    "# Intersect the water feature boundaries with the park boundary\n",
    "#bluespace['shared_boundary'] = bluespace['boundary'].apply(lambda b: b.intersection(park_boundary))\n",
    "\n",
    "# Calculate the length of the shared boundary for each water feature\n",
    "#bluespace['shared_length'] = bluespace['shared_boundary'].apply(lambda b: b.length)\n",
    "\n",
    "# Calculate the percentage of the water feature's boundary that is shared with the park boundary\n",
    "#bluespace['perimeter'] = bluespace['boundary'].apply(lambda b: b.length)\n",
    "#bluespace['shared_percentage'] = (bluespace['shared_length'] / bluespace['perimeter']) * 100\n",
    "\n",
    "# Filter water features where at least 80% of the boundary is adjacent to parks\n",
    "#adj_bluespace = bluespace[bluespace['shared_percentage'] >= 80]\n",
    "\n",
    "# Optional: Add these selected water features back to the parks dataset\n",
    "#parks_with_water = gpd.GeoDataFrame(pd.concat([filtered_parks, adj_bluespace], ignore_index=True))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11060, 19)\n",
      "(10415, 15)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the result\n",
    "print(parks_with_water.shape)\n",
    "print(filtered_parks.shape)\n",
    "\n",
    "#parks_with_water = parks_with_water.drop(columns=['boundary', 'shared_boundary'])\n",
    "#parks_with_water.set_geometry('geometry', inplace=True)\n",
    "parks_with_water.to_file('C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\parks_w_water.geojson')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
