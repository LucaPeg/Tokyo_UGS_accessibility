{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBTAINING PARK LAYER\n",
    "The goal of this notebook is to produce a final urban green areas layer to performe accessibility analysis. <br>\n",
    "I will use as a baseline the R03土地利用現況 file, which authored by Tokyo Metropolitan Government (opendata.metro.tokyo).The issue with these data is that they include also cemeteries and sports facilities as parks. My solution is the following:\n",
    "- Extract from the land use data the parks (LU_1: 300)\n",
    "- Query OSM for cemeteries, graveyeards, pitches and sport centers.\n",
    "- First merge (dissolve) the extracted land park polygons\n",
    "- Exclude the UGS polygons which overlap to the OSM layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugs_path = \"C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\R03\\\\R03土地利用現況.shp\"\n",
    "unwanted_path = \"C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\unwanted_features.geojson\" # obtained through OSM\n",
    "landuse = gpd.read_file(ugs_path) # layer of Tokyo landuse\n",
    "unwanted = gpd.read_file(unwanted_path) # layer of cemeteries, graveyeards and sport facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess parks data\n",
    "- Fix CRS\n",
    "- Merge adjecent parks\n",
    "- Create index and recompute areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original CRS: EPSG:6677\n",
      "geometry attribute name: geometry\n"
     ]
    }
   ],
   "source": [
    "print(f\"original CRS: {landuse.crs}\") # originally the data is in EPSG:6677\n",
    "landuse = landuse.to_crs(epsg=32654) # I do this because I need a CRS that keeps information about distance to compute the buffers\n",
    "unwanted = unwanted.to_crs(epsg=32654)\n",
    "print(f\"geometry attribute name: {landuse.geometry.name}\") # this gives the name of the attibute corresponding to the geometry column (a GeoSeries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total polygons in the land usage dataset: 815736\n",
      "Number of parks' polygons: 14030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucap\\anaconda3\\envs\\spatial\\Lib\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "# filter only the parks from the land use dataframe\n",
    "print(f\"Total polygons in the land usage dataset: {landuse.shape[0]}\")\n",
    "parks = landuse[landuse[\"LU_1\"] == 300] # 300 identifies parks\n",
    "print(f\"Number of parks' polygons: {parks.shape[0]}\")\n",
    "\n",
    "# create a new index and update the areas\n",
    "parks['park_id'] = range(1,len(parks)+1)\n",
    "parks.set_index(parks.park_id)\n",
    "parks['AREA'] = parks.geometry.area\n",
    "parks = parks.rename(columns={'AREA':'area'})\n",
    "initial_parks = parks.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove unwanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1546 parks removed\n"
     ]
    }
   ],
   "source": [
    "# remove features that are mostly unwanted\n",
    "overlap = gpd.overlay(parks, unwanted, how='intersection')\n",
    "overlap['overlap_area'] = overlap.geometry.area\n",
    "tot_overlap = overlap.groupby('park_id')['overlap_area'].sum().reset_index()\n",
    "parks = parks.merge(tot_overlap, on='park_id', how='left')\n",
    "parks['overlap_area'] = parks['overlap_area'].fillna(0)\n",
    "parks['ov_percentage'] = (parks['overlap_area']/parks['area'])* 100\n",
    "filtered_parks = parks[parks['ov_percentage'] <= 50]\n",
    "parks_after_removal = filtered_parks.shape[0]\n",
    "n_parks_removed1 = initial_parks - parks_after_removal\n",
    "print(f\"{n_parks_removed1} parks removed\")\n",
    "\n",
    "# export filtered data\n",
    "# filtered_parks.to_file(\"C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\filtered_parks.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell block used to be above (before checking for unwanted areas)\n",
    "I will now try to keep it here to see whether it is better. The rationale is the following: <br>\n",
    "The following process dissolves and explodes the UGS. Therefore by desing the average area of the parks will increase after the process.\n",
    "It is therefore possible that by moving this process _after_ removing unwanted polygons, more unwanted polygons will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parks' after merging: 10517\n",
      "Step 1 eliminated 1546\n",
      "Step 2 eliminated 1967\n"
     ]
    }
   ],
   "source": [
    "# merge the adjcent parks into a single entity\n",
    "filtered_parks = filtered_parks.dissolve()\n",
    "filtered_parks = filtered_parks.explode()\n",
    "print(f\"Number of parks' after merging: {filtered_parks.shape[0]}\")\n",
    "print(f\"Step 1 eliminated {n_parks_removed1}\")\n",
    "print(f\"Step 2 eliminated {parks_after_removal - filtered_parks.shape[0]}\")\n",
    "filtered_parks['area'] = filtered_parks.geometry.area # the value changed after merging\n",
    "\n",
    "filtered_parks.to_file(\"C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\filtered_2.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt to include bodies of water\n",
    "THIS IS COMPELTELY WRONG NOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main idea: a body of water should be included only if most of its perimeter touches the parks. <br>\n",
    "This would allow for ponds within parks to be included into the park itself, however huge rivers would be omitted. <br>\n",
    "Some complications:\n",
    "- Parks can be divided into multiple polygons, so it is important to consider that the total perimiter touching all polygons that compose the park is what matters to evaluate a bluespace's inclusion in the dataset.\n",
    "- My solution is dissolving all parks in one single polygon. However bluespace that touches multiple parks may wrongly be added to the dataset.\n",
    "\n",
    "HOWEVER: many smaller bodies of water are already included into the parks' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucap\\anaconda3\\envs\\spatial\\Lib\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "c:\\Users\\lucap\\anaconda3\\envs\\spatial\\Lib\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "c:\\Users\\lucap\\anaconda3\\envs\\spatial\\Lib\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "c:\\Users\\lucap\\anaconda3\\envs\\spatial\\Lib\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "c:\\Users\\lucap\\anaconda3\\envs\\spatial\\Lib\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "# Dissolve the parks into a single geometry (unified park boundary)\n",
    "unipark = filtered_parks.dissolve()\n",
    "\n",
    "# Create a single boundary geometry for the parks\n",
    "park_boundary = unipark.geometry.boundary.union_all()  # This creates a single LineString or MultiLineString\n",
    "\n",
    "# Apply buffering to the park boundary (adjust buffer size as needed, e.g., 10 meters)\n",
    "buffered_park_boundary = park_boundary.buffer(2)\n",
    "\n",
    "# Filter the bodies of water (LU_1 = 700)\n",
    "bluespace = landuse[landuse['LU_1'] == 700]\n",
    "\n",
    "# Calculate the boundary of each body of water\n",
    "bluespace['boundary'] = bluespace.geometry.boundary\n",
    "\n",
    "# Intersect the buffered park boundary with the water feature boundaries\n",
    "bluespace['shared_boundary'] = bluespace['boundary'].apply(lambda b: b.intersection(buffered_park_boundary))\n",
    "\n",
    "# Calculate the length of the shared boundary for each water feature\n",
    "bluespace['shared_length'] = bluespace['shared_boundary'].apply(lambda b: b.length)\n",
    "\n",
    "# Calculate the percentage of the water feature's boundary that is shared with the buffered park boundary\n",
    "bluespace['perimeter'] = bluespace['boundary'].apply(lambda b: b.length)\n",
    "bluespace['shared_percentage'] = (bluespace['shared_length'] / bluespace['perimeter']) * 100\n",
    "\n",
    "# Filter water features where at least 80% of the boundary is adjacent to the park boundary or its buffer\n",
    "adj_bluespace = bluespace[bluespace['shared_percentage'] >= 80]\n",
    "\n",
    "# Optional: Add these selected water features back to the parks dataset\n",
    "parks_with_water = gpd.GeoDataFrame(pd.concat([filtered_parks, adj_bluespace], ignore_index=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the first iteration of adding the bodies of water inside parks. <br>\n",
    "It worked, but without buffering \"only\" 200+ polygons were added. Some important absents were the Ueno park's ponds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve the parks into a single geometry (unified park boundary)\n",
    "#unipark = filtered_parks.dissolve()\n",
    "\n",
    "# Create a single boundary geometry for the parks\n",
    "#park_boundary = unipark.geometry.boundary.union_all()  # This creates a single LineString or MultiLineString\n",
    "\n",
    "# Filter the bodies of water (LU_1 = 700)\n",
    "#bluespace = landuse[landuse['LU_1'] == 700]\n",
    "\n",
    "# Calculate the boundary of each body of water\n",
    "#bluespace['boundary'] = bluespace.geometry.boundary\n",
    "\n",
    "# Intersect the water feature boundaries with the park boundary\n",
    "#bluespace['shared_boundary'] = bluespace['boundary'].apply(lambda b: b.intersection(park_boundary))\n",
    "\n",
    "# Calculate the length of the shared boundary for each water feature\n",
    "#bluespace['shared_length'] = bluespace['shared_boundary'].apply(lambda b: b.length)\n",
    "\n",
    "# Calculate the percentage of the water feature's boundary that is shared with the park boundary\n",
    "#bluespace['perimeter'] = bluespace['boundary'].apply(lambda b: b.length)\n",
    "#bluespace['shared_percentage'] = (bluespace['shared_length'] / bluespace['perimeter']) * 100\n",
    "\n",
    "# Filter water features where at least 80% of the boundary is adjacent to parks\n",
    "#adj_bluespace = bluespace[bluespace['shared_percentage'] >= 80]\n",
    "\n",
    "# Optional: Add these selected water features back to the parks dataset\n",
    "#parks_with_water = gpd.GeoDataFrame(pd.concat([filtered_parks, adj_bluespace], ignore_index=True))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10944, 21)\n",
      "(10517, 15)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the result\n",
    "print(parks_with_water.shape)\n",
    "print(filtered_parks.shape)\n",
    "\n",
    "parks_with_water = parks_with_water.drop(columns=['boundary', 'shared_boundary'])\n",
    "#parks_with_water.set_geometry('geometry', inplace=True)\n",
    "parks_with_water.to_file('C:\\\\Users\\\\lucap\\\\Documents\\\\thesis_work\\\\data\\\\ugs\\\\parks_w_water2.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO dissolve and explode again\n",
    "# TODO add meji jingu and chiyoda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
